{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataCleaning&Extraction-December.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZYD8momopwX"
      },
      "source": [
        "# Import PyDrive and associated libraries\n",
        "# This only needs to be done once per notebook\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "# This only needs to be done once per notebook\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pYcLKMdxGu8"
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1DXI9nkm4uJpC4Gd0Zpb1-Sr_7ZdJ2531' # Check your own ID in GDrive\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "# Save file in Colab memory\n",
        "downloaded.GetContentFile('tweets1.csv')  \n",
        "\n",
        "# Rest of the December files\n",
        "file_id = '1xYBggzLZnQoHEUNOSS4wrJAOOlLlGMuu' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets2.csv')  \n",
        "file_id = '1ydSToFadEGQf3KnV9NKPJCo-f3XW7Mh6' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets3.csv')  \n",
        "file_id = '1TOti_C6oGE5fD-EBs8MfLYjWIfc4kXTm' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets4.csv')  \n",
        "file_id = '13hR5l-NwdXLY6IX8hVcC_2LJ4UKae3bA' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets5.csv')  \n",
        "file_id = '1BKif6OeW6hhN0TVVnqGMS_yxipXtrbG5' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets6.csv')  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-moV4XXRvBS"
      },
      "source": [
        "# Dec 28 Data\n",
        "file_id = '1gkvDFrUrzDTmtpkqOKM6FcS3FrbPJmYu' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets7.csv')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2uG-eJxchKu"
      },
      "source": [
        "# Dec 31 Data\n",
        "file_id = '1KPckqUIXCXyC1doTvJMEaWRWHVgoQ9Kn' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets8.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLLELwUHyH98"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX6CtYgwyRTJ"
      },
      "source": [
        "df1 = pd.read_csv('tweets1.csv',\n",
        "                 lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc40xbiJyYFm"
      },
      "source": [
        "# Cut short data to 1st of December\n",
        "df1 = df1.loc[:234064]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuO657A0ysKz"
      },
      "source": [
        "df1 = df1[df1[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W8dhWs70lRg",
        "outputId": "ebdb3265-bc5f-49ef-9fad-c605037a7530"
      },
      "source": [
        "pip install contractions emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.5.0.tar.gz (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 44.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 52.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji, pyahocorasick\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.5.0-py3-none-any.whl size=187457 sha256=d8f8a76784b8e5ac5a2dea8d0daae2a476c98985aa0d2062d5579adbf4b1a905\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b5/f6/b39abf14e94b3d6640613bbe630a66c10ccf7a12882d064fb5\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85455 sha256=ebe75173551ad1477da1a639f3368d50cf44312c1530115c741e8c95e631affe\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built emoji pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, emoji, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 emoji-1.5.0 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMRfYBM71BZr"
      },
      "source": [
        "# Cleaning Datetime, Text, Tokenization, Stopwords, Lemmatization\n",
        "import matplotlib.pyplot as plt\n",
        "import re, contractions, emoji\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def clean_date(date_obj):\n",
        "  for x in range(0, len(date_obj)):\n",
        "    if date_obj[x] == \" \":\n",
        "      break\n",
        "  date_obj = date_obj[:x]\n",
        "  return date_obj\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  ## Twitter Features\n",
        "    # replace retweet\n",
        "  tweet = re.sub('RT\\s+', \"\", tweet )\n",
        "    # replace user tag\n",
        "  tweet = re.sub('\\B@\\w+', \"\", tweet)\n",
        "    # replace url\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', \"\", tweet)\n",
        "    # replace hashtag\n",
        "  tweet = re.sub('#+', \"\", tweet)\n",
        "\n",
        "  ## Word Features\n",
        "    # lower case\n",
        "  tweet = tweet.lower()\n",
        "    # replace contractions\n",
        "  tweet = contractions.fix(tweet)\n",
        "    # replace punctuation repetition\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', \"\", tweet)\n",
        "    # replace word repetition\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "    # replace emojis\n",
        "  tweet = emoji.demojize(tweet)\n",
        "\n",
        "  return tweet\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUdYgVLJ4hGw"
      },
      "source": [
        "df1[\"Datetime\"] = df1[\"Datetime\"].apply(clean_date)\n",
        "df1[\"Clean Text\"] = df1[\"Text\"].apply(process_tweet)\n",
        "df1['Clean Text']= df1['Clean Text'].str.replace('[^\\w\\s]','')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKR23Z7O4zUm",
        "outputId": "413b51c0-1a23-41ff-e350-9531d4b6fe34"
      },
      "source": [
        "# Removing Stop Words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvdTJHpJ5Pzp"
      },
      "source": [
        "df1['Text Stop']  = df1['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "aY56brMo5jRo",
        "outputId": "d0b5393f-bbae-40a5-cb4d-915ba18710b2"
      },
      "source": [
        "#Tokenization of Tweets\n",
        "import textblob           \n",
        "from textblob import TextBlob\n",
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "df1['Text Token'] = df1['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df1[['Clean Text', 'Text Stop', 'Text Token']][0:9]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clean Text</th>\n",
              "      <th>Text Stop</th>\n",
              "      <th>Text Token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>trump snubbed by pfizer and moderna as he prep...</td>\n",
              "      <td>trump snubbed pfizer moderna prepares host whi...</td>\n",
              "      <td>[trump, snubbed, pfizer, moderna, prepares, ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>justin trudeau says pfizers covid19 vaccine is...</td>\n",
              "      <td>justin trudeau says pfizers covid19 vaccine co...</td>\n",
              "      <td>[justin, trudeau, says, pfizers, covid19, vacc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>please read this before telling me the covid v...</td>\n",
              "      <td>please read telling covid vaccine going give e...</td>\n",
              "      <td>[please, read, telling, covid, vaccine, going,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alberta to receive 3900 doses of pfizers covid...</td>\n",
              "      <td>alberta receive 3900 doses pfizers covid19 vac...</td>\n",
              "      <td>[alberta, receive, 3900, doses, pfizers, covid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>breaking news the pmo has confirmed that liber...</td>\n",
              "      <td>breaking news pmo confirmed liberal politician...</td>\n",
              "      <td>[breaking, news, pmo, confirmed, liberal, poli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>do not be it is the wrong time to be upset wi...</td>\n",
              "      <td>wrong time upset going america pray vaccine co...</td>\n",
              "      <td>[wrong, time, upset, going, america, pray, vac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>last_track_buttonrewind  michigan chief medica...</td>\n",
              "      <td>last_track_buttonrewind michigan chief medical...</td>\n",
              "      <td>[last_track_buttonrewind, michigan, chief, med...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>is this why they want to call it the trump va...</td>\n",
              "      <td>want call trump vaccine would like trumps heal...</td>\n",
              "      <td>[want, call, trump, vaccine, would, like, trum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>inspirational account of the determination of ...</td>\n",
              "      <td>inspirational account determination amp others...</td>\n",
              "      <td>[inspirational, account, determination, amp, o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Clean Text  ...                                         Text Token\n",
              "0  trump snubbed by pfizer and moderna as he prep...  ...  [trump, snubbed, pfizer, moderna, prepares, ho...\n",
              "1  justin trudeau says pfizers covid19 vaccine is...  ...  [justin, trudeau, says, pfizers, covid19, vacc...\n",
              "3  please read this before telling me the covid v...  ...  [please, read, telling, covid, vaccine, going,...\n",
              "4  alberta to receive 3900 doses of pfizers covid...  ...  [alberta, receive, 3900, doses, pfizers, covid...\n",
              "5  breaking news the pmo has confirmed that liber...  ...  [breaking, news, pmo, confirmed, liberal, poli...\n",
              "6   do not be it is the wrong time to be upset wi...  ...  [wrong, time, upset, going, america, pray, vac...\n",
              "7  last_track_buttonrewind  michigan chief medica...  ...  [last_track_buttonrewind, michigan, chief, med...\n",
              "8   is this why they want to call it the trump va...  ...  [want, call, trump, vaccine, would, like, trum...\n",
              "9  inspirational account of the determination of ...  ...  [inspirational, account, determination, amp, o...\n",
              "\n",
              "[9 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz7u1vpm6fQb",
        "outputId": "3a1cbd5c-12ba-4769-f70b-a5fe216c7816"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ0AD1TH6B9X"
      },
      "source": [
        "lmtzr = nltk.WordNetLemmatizer()\n",
        "def lemmatizer(text):\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "    return text\n",
        "\n",
        "df1['Text Lemmatized'] = df1['Text Token'].apply(lambda x: lemmatizer(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3xABp7l6wKr"
      },
      "source": [
        "def remove_spaces(list1):\n",
        "  list1 = \" \".join(list1).split()\n",
        "  return list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_QWIE9O_w40"
      },
      "source": [
        "df1[\"Text Lemmatized\"] = df1[\"Text Lemmatized\"].apply(remove_spaces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkOrV14LACNy"
      },
      "source": [
        "df1 = df1.drop(columns=['Text Stop', 'Text Token'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJGFvkgA3nG"
      },
      "source": [
        "# Preprocess all the rest of the data\n",
        "df2 = pd.read_csv('tweets2.csv')\n",
        "df2 = df2[df2[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df2[\"Datetime\"] = df2[\"Datetime\"].apply(clean_date)\n",
        "df2[\"Clean Text\"] = df2[\"Text\"].apply(process_tweet)\n",
        "df2['Text Stop']  = df2['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df2['Text Token'] = df2['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df2['Text Lemmatized'] = df2['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df2[\"Text Lemmatized\"] = df2[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df2['Clean Text']= df2['Clean Text'].str.replace('[^\\w\\s]','')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kVfNeogGhPC"
      },
      "source": [
        "df2 = df2.drop(columns=['Text Stop', 'Text Token'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1HTbmpuFogq"
      },
      "source": [
        "df3 = pd.read_csv('tweets3.csv')\n",
        "df3 = df3[df3[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df3[\"Datetime\"] = df3[\"Datetime\"].apply(clean_date)\n",
        "df3[\"Clean Text\"] = df3[\"Text\"].apply(process_tweet)\n",
        "df3['Text Stop']  = df3['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df3['Text Token'] = df3['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df3['Text Lemmatized'] = df3['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df3[\"Text Lemmatized\"] = df3[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df3 = df3.drop(columns=['Text Stop', 'Text Token'])\n",
        "df3['Clean Text']= df3['Clean Text'].str.replace('[^\\w\\s]','')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnWdJs6OF1RD"
      },
      "source": [
        "df4 = pd.read_csv('tweets4.csv')\n",
        "df4 = df4[df4[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df4[\"Datetime\"] = df4[\"Datetime\"].apply(clean_date)\n",
        "df4[\"Clean Text\"] = df4[\"Text\"].apply(process_tweet)\n",
        "df4['Text Stop']  = df4['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df4['Text Token'] = df4['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df4['Text Lemmatized'] = df4['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df4[\"Text Lemmatized\"] = df4[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df4 = df4.drop(columns=['Text Stop', 'Text Token'])\n",
        "df4['Clean Text']= df4['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYVt7NetGD57"
      },
      "source": [
        "df5 = pd.read_csv('tweets5.csv')\n",
        "df5 = df5[df5[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df5[\"Datetime\"] = df5[\"Datetime\"].apply(clean_date)\n",
        "df5[\"Clean Text\"] = df5[\"Text\"].apply(process_tweet)\n",
        "df5['Text Stop']  = df5['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df5['Text Token'] = df5['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df5['Text Lemmatized'] = df5['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df5[\"Text Lemmatized\"] = df5[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df5 = df5.drop(columns=['Text Stop', 'Text Token'])\n",
        "df5['Clean Text']= df5['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "insTj2IzGGtA"
      },
      "source": [
        "df6 = pd.read_csv('tweets6.csv')\n",
        "df6 = df6[df6[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df6[\"Datetime\"] = df6[\"Datetime\"].apply(clean_date)\n",
        "df6[\"Clean Text\"] = df6[\"Text\"].apply(process_tweet)\n",
        "df6['Text Stop']  = df6['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df6['Text Token'] = df6['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df6['Text Lemmatized'] = df6['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df6[\"Text Lemmatized\"] = df6[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df6 = df6.drop(columns=['Text Stop', 'Text Token'])\n",
        "df6['Clean Text']= df6['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUe74lpCO9KD"
      },
      "source": [
        "df7 = pd.read_csv('tweets7.csv')\n",
        "df7 = df7.loc[:25943]\n",
        "df7 = df7[df7[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df7[\"Datetime\"] = df7[\"Datetime\"].apply(clean_date)\n",
        "df7[\"Clean Text\"] = df7[\"Text\"].apply(process_tweet)\n",
        "df7['Text Stop']  = df7['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df7['Text Token'] = df7['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df7['Text Lemmatized'] = df7['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df7[\"Text Lemmatized\"] = df7[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df7 = df7.drop(columns=['Text Stop', 'Text Token'])\n",
        "df7['Clean Text']= df7['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anku43F_cqec"
      },
      "source": [
        "df8 = pd.read_csv('tweets8.csv')\n",
        "df8 = df8[df8[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df8[\"Datetime\"] = df8[\"Datetime\"].apply(clean_date)\n",
        "df8[\"Clean Text\"] = df8[\"Text\"].apply(process_tweet)\n",
        "df8['Text Stop']  = df8['Clean Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df8['Text Token'] = df8['Text Stop'].apply(lambda x: tokenization(x.lower()))\n",
        "df8['Text Lemmatized'] = df8['Text Token'].apply(lambda x: lemmatizer(x))\n",
        "df8[\"Text Lemmatized\"] = df8[\"Text Lemmatized\"].apply(remove_spaces)\n",
        "df8 = df8.drop(columns=['Text Stop', 'Text Token'])\n",
        "df8['Clean Text']= df8['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "Ny4yrNpZTDs-",
        "outputId": "aa505a04-4bc7-46c2-82f7-0d77ad115761"
      },
      "source": [
        "# Collect all data from December\n",
        "df_dec = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8]).drop('Text Lemmatized', axis=1)\n",
        "df_dec = df_dec.sort_values(by=['Datetime'])\n",
        "df_dec.drop_duplicates(subset =[\"Datetime\", \"Text\", \"Like Count\", \"Username\"], keep = False, inplace = True)\n",
        "df_dec = df_dec.reset_index()\n",
        "del df_dec['index']\n",
        "df_dec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Text</th>\n",
              "      <th>Username</th>\n",
              "      <th>Like Count</th>\n",
              "      <th>Display Name</th>\n",
              "      <th>Clean Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>Union Health Secretary Rajesh Bhushan said the...</td>\n",
              "      <td>ThePrintIndia</td>\n",
              "      <td>6</td>\n",
              "      <td>ThePrintIndia</td>\n",
              "      <td>union health secretary rajesh bhushan said the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>First Covid vaccines to be offered to health w...</td>\n",
              "      <td>walk4chang_sue</td>\n",
              "      <td>0</td>\n",
              "      <td>Sue Walker</td>\n",
              "      <td>first covid vaccines to be offered to health w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>The AP reported that An influential Gov't Pane...</td>\n",
              "      <td>DogginTrump</td>\n",
              "      <td>95</td>\n",
              "      <td>WTFGOP?</td>\n",
              "      <td>the ap reported that an influential govt panel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>CDC vaccine advisers recommend health care sta...</td>\n",
              "      <td>hazeldfernandez</td>\n",
              "      <td>0</td>\n",
              "      <td>нαzєℓ</td>\n",
              "      <td>cdc vaccine advisers recommend health care sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>CDC panel says health workers, nursing homes w...</td>\n",
              "      <td>MrBig305</td>\n",
              "      <td>0</td>\n",
              "      <td>Just Me  - ALL GODS PAST &amp; PRESENT, ARE IMAGINARY</td>\n",
              "      <td>cdc panel says health workers nursing homes wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080748</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>my brother (paramedic) got his first covid vac...</td>\n",
              "      <td>stringoflightsx</td>\n",
              "      <td>15</td>\n",
              "      <td>mary</td>\n",
              "      <td>my brother paramedic got his first covid vacci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080749</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>The line to get the COVID-19 vaccine is over 3...</td>\n",
              "      <td>pepeleffew</td>\n",
              "      <td>0</td>\n",
              "      <td>Pepe Leffew</td>\n",
              "      <td>the line to get the covid19 vaccine is over 3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080750</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>My wife got the Covid vaccine today! Very exci...</td>\n",
              "      <td>It_me_Greg</td>\n",
              "      <td>0</td>\n",
              "      <td>Greg with two g's</td>\n",
              "      <td>my wife got the covid vaccine today very exciting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080751</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>They spent 12 years solving a puzzle. It yield...</td>\n",
              "      <td>JiggySevilla</td>\n",
              "      <td>0</td>\n",
              "      <td>Jiggy Sevilla</td>\n",
              "      <td>they spent 12 years solving a puzzle it yielde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080752</th>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>dublinlive_ie: 'Entire population unlikely to ...</td>\n",
              "      <td>Fovle_Online</td>\n",
              "      <td>0</td>\n",
              "      <td>Fovle</td>\n",
              "      <td>dublinlive_ie entire population unlikely to re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1080753 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Datetime  ...                                         Clean Text\n",
              "0        2020-12-01  ...  union health secretary rajesh bhushan said the...\n",
              "1        2020-12-01  ...  first covid vaccines to be offered to health w...\n",
              "2        2020-12-01  ...  the ap reported that an influential govt panel...\n",
              "3        2020-12-01  ...  cdc vaccine advisers recommend health care sta...\n",
              "4        2020-12-01  ...  cdc panel says health workers nursing homes wi...\n",
              "...             ...  ...                                                ...\n",
              "1080748  2020-12-31  ...  my brother paramedic got his first covid vacci...\n",
              "1080749  2020-12-31  ...  the line to get the covid19 vaccine is over 3 ...\n",
              "1080750  2020-12-31  ...  my wife got the covid vaccine today very exciting\n",
              "1080751  2020-12-31  ...  they spent 12 years solving a puzzle it yielde...\n",
              "1080752  2020-12-31  ...  dublinlive_ie entire population unlikely to re...\n",
              "\n",
              "[1080753 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv-SvwcWTb3J"
      },
      "source": [
        "df_dec.to_csv('dec_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bscGbUVor7Cu",
        "outputId": "13dea135-fd71-4cf4-ea6f-7e3f15b18496"
      },
      "source": [
        "df_dec.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Datetime        1080753\n",
              "Text            1080753\n",
              "Username        1080752\n",
              "Like Count      1080753\n",
              "Display Name    1080681\n",
              "Clean Text      1080753\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qu82jhNsc2h",
        "outputId": "c8db3908-f980-4268-dc5d-706efcfa0271"
      },
      "source": [
        "datatypes = df_dec.dtypes\n",
        "datatypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Datetime        object\n",
              "Text            object\n",
              "Username        object\n",
              "Like Count       int64\n",
              "Display Name    object\n",
              "Clean Text      object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}