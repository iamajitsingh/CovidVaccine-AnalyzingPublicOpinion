{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataCleaning&Extraction-September.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeJVAy8kYlZu"
      },
      "source": [
        "# Import PyDrive and associated libraries\n",
        "# This only needs to be done once per notebook\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "# This only needs to be done once per notebook\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiinkgCiYt2p"
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1HVHn0_VzSpgNx-YW1OXHySvmUWclt_Fn' # Check your own ID in GDrive\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "# Save file in Colab memory\n",
        "downloaded.GetContentFile('tweets1.csv')  \n",
        "\n",
        "# Rest of the December files\n",
        "file_id = '10oXOl6pif-wL1ekBdIZX7vMYyvjnidb0' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets2.csv')  \n",
        "file_id = '1Ea1R2ffHp_HF7yPGnyvyG8YH3ONbLQDJ' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets3.csv')  \n",
        "file_id = '1AKBiyI3PbWMdFtntUTqxCZdDsOX19Z45' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets4.csv')  \n",
        "file_id = '16637j0epK_oFu-mZWQO3-2RZXnrSq5v9' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets5.csv')  \n",
        "file_id = '1EIZq97imBRQAR9otTjTofPu9CFAi_EWQ' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets6.csv') \n",
        "file_id = '1WYSEHB9HFgqeT4zVQG6TQ-x_D0NUmCgf' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets7.csv') \n",
        "file_id = '1ar5ShGJ2hQFBXKx8xDW47TepynUT8PKs' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets8.csv') \n",
        "file_id = '1a0UXldYnD8KUqf3ZkGIl325OnO7MIuXQ' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets9.csv')  \n",
        "file_id = '1im04FKkuy5sh30fu1UsZbCNOrtqz0uaS' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets10.csv')  \n",
        "file_id = '1qdd471yWAri5hc4eJo7XkvzXkSH4GwVT' \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets11.csv')  \n",
        "file_id = '12DBtX1_pKR6zzWc46I6Fj8YF5wKHCqb5'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets12.csv')  \n",
        "file_id = '1KBa-qwFmdymq2Ha7JSd4pIJ9-JksuN4h'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('tweets13.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYd42aqSaT1O"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gmj5qwybCqZ"
      },
      "source": [
        "df1 = pd.read_csv('tweets1.csv',\n",
        "                 lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLX8rdEXbFjm"
      },
      "source": [
        "df1 = df1[df1[\"Language\\r\"]==\"en\\r\"].drop(['Unnamed: 0', 'Tweet Id', 'Language\\r'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7aV_jFfD5-",
        "outputId": "8b4775bf-ff57-4e51-b4cb-9cf67f47e418"
      },
      "source": [
        "pip install contractions emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.5.0.tar.gz (185 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185 kB 32.1 MB/s \n",
            "\u001b[?25hCollecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 284 kB 70.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji, pyahocorasick\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.5.0-py3-none-any.whl size=187457 sha256=74042b9685742de3ce5389eece5b1e63c577a749708e8db00a187acaa74ebf09\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b5/f6/b39abf14e94b3d6640613bbe630a66c10ccf7a12882d064fb5\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85453 sha256=ec069444e30208fc05f05e0c5cadb6be45c8da9fbcccfd3bf194734f17a138fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built emoji pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, emoji, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 emoji-1.5.0 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCyZuyzxfZbE"
      },
      "source": [
        "# Cleaning Datetime, Text, Tokenization, Stopwords, Lemmatization\n",
        "import matplotlib.pyplot as plt\n",
        "import re, contractions, emoji\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def clean_date(date_obj):\n",
        "  for x in range(0, len(date_obj)):\n",
        "    if date_obj[x] == \" \":\n",
        "      break\n",
        "  date_obj = date_obj[:x]\n",
        "  return date_obj\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  ## Twitter Features\n",
        "    # replace retweet\n",
        "  tweet = re.sub('RT\\s+', \"\", tweet )\n",
        "    # replace user tag\n",
        "  tweet = re.sub('\\B@\\w+', \"\", tweet)\n",
        "    # replace url\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', \"\", tweet)\n",
        "    # replace hashtag\n",
        "  tweet = re.sub('#+', \"\", tweet)\n",
        "\n",
        "  ## Word Features\n",
        "    # lower case\n",
        "  tweet = tweet.lower()\n",
        "    # replace contractions\n",
        "  tweet = contractions.fix(tweet)\n",
        "    # replace punctuation repetition\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', \"\", tweet)\n",
        "    # replace word repetition\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "    # replace emojis\n",
        "  tweet = emoji.demojize(tweet)\n",
        "\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0aeSGn1fdEb"
      },
      "source": [
        "df1[\"Datetime\"] = df1[\"Datetime\"].apply(clean_date)\n",
        "df1[\"Clean Text\"] = df1[\"Text\"].apply(process_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBW1FXqZfopu"
      },
      "source": [
        "df1['Clean Text']= df1['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmT0Ri1gghUp"
      },
      "source": [
        "# Preprocess all the rest of the data\n",
        "df2 = pd.read_csv('tweets2.csv')\n",
        "df2 = df2[df2[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df2[\"Datetime\"] = df2[\"Datetime\"].apply(clean_date)\n",
        "df2[\"Clean Text\"] = df2[\"Text\"].apply(process_tweet)\n",
        "df2['Clean Text']= df2['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIw2s43xhEbn"
      },
      "source": [
        "df3 = pd.read_csv('tweets3.csv')\n",
        "df3 = df3[df3[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df3[\"Datetime\"] = df3[\"Datetime\"].apply(clean_date)\n",
        "df3[\"Clean Text\"] = df3[\"Text\"].apply(process_tweet)\n",
        "df3['Clean Text']= df3['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50wi-W8ChMsZ"
      },
      "source": [
        "df4 = pd.read_csv('tweets4.csv')\n",
        "df4 = df4[df4[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df4[\"Datetime\"] = df4[\"Datetime\"].apply(clean_date)\n",
        "df4[\"Clean Text\"] = df4[\"Text\"].apply(process_tweet)\n",
        "df4['Clean Text']= df4['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ITiMzw4hNJG"
      },
      "source": [
        "df5 = pd.read_csv('tweets5.csv')\n",
        "df5 = df5[df5[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df5[\"Datetime\"] = df5[\"Datetime\"].apply(clean_date)\n",
        "df5[\"Clean Text\"] = df5[\"Text\"].apply(process_tweet)\n",
        "df5['Clean Text']= df5['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L8-CqRmhNa6"
      },
      "source": [
        "df6 = pd.read_csv('tweets6.csv')\n",
        "df6 = df6[df6[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df6[\"Datetime\"] = df6[\"Datetime\"].apply(clean_date)\n",
        "df6[\"Clean Text\"] = df6[\"Text\"].apply(process_tweet)\n",
        "df6['Clean Text']= df6['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkttIfZahx12"
      },
      "source": [
        "df7 = pd.read_csv('tweets7.csv')\n",
        "df7 = df7[df7[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df7[\"Datetime\"] = df7[\"Datetime\"].apply(clean_date)\n",
        "df7[\"Clean Text\"] = df7[\"Text\"].apply(process_tweet)\n",
        "df7['Clean Text']= df7['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG3lbgFnhzRk"
      },
      "source": [
        "df8 = pd.read_csv('tweets8.csv')\n",
        "df8 = df8[df8[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df8[\"Datetime\"] = df8[\"Datetime\"].apply(clean_date)\n",
        "df8[\"Clean Text\"] = df8[\"Text\"].apply(process_tweet)\n",
        "df8['Clean Text']= df8['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm2C_TYUhzn5"
      },
      "source": [
        "df9 = pd.read_csv('tweets9.csv')\n",
        "df9 = df9[df9[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df9[\"Datetime\"] = df9[\"Datetime\"].apply(clean_date)\n",
        "df9[\"Clean Text\"] = df9[\"Text\"].apply(process_tweet)\n",
        "df9['Clean Text']= df9['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DGUQROshz8L"
      },
      "source": [
        "df10 = pd.read_csv('tweets10.csv')\n",
        "df10 = df10[df10[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df10[\"Datetime\"] = df10[\"Datetime\"].apply(clean_date)\n",
        "df10[\"Clean Text\"] = df10[\"Text\"].apply(process_tweet)\n",
        "df10['Clean Text']= df10['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7oZ_j8zh0K_"
      },
      "source": [
        "df11 = pd.read_csv('tweets11.csv')\n",
        "df11 = df11[df11[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df11[\"Datetime\"] = df11[\"Datetime\"].apply(clean_date)\n",
        "df11[\"Clean Text\"] = df11[\"Text\"].apply(process_tweet)\n",
        "df11['Clean Text']= df11['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBPsGN00h0aM"
      },
      "source": [
        "df12 = pd.read_csv('tweets12.csv')\n",
        "df12 = df12[df12[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df12[\"Datetime\"] = df12[\"Datetime\"].apply(clean_date)\n",
        "df12[\"Clean Text\"] = df12[\"Text\"].apply(process_tweet)\n",
        "df12['Clean Text']= df12['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYqyutB5h0qM"
      },
      "source": [
        "df13 = pd.read_csv('tweets13.csv')\n",
        "df13 = df13[df13[\"Language\"]==\"en\"].drop(['Unnamed: 0', 'Tweet Id', 'Language'],axis=1)\n",
        "df13[\"Datetime\"] = df13[\"Datetime\"].apply(clean_date)\n",
        "df13[\"Clean Text\"] = df13[\"Text\"].apply(process_tweet)\n",
        "df13['Clean Text']= df13['Clean Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "Y5otS9-Oif4-",
        "outputId": "2f28effb-56a4-4a32-93c1-94b8c87b5180"
      },
      "source": [
        "# Collect all data from september\n",
        "df_sep = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13])\n",
        "df_sep = df_sep.sort_values(by=['Datetime'])\n",
        "df_sep.drop_duplicates(subset =[\"Datetime\", \"Text\", \"Like Count\", \"Username\"], keep = False, inplace = True)\n",
        "df_sep = df_sep.reset_index()\n",
        "del df_sep['index']\n",
        "df_sep"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Text</th>\n",
              "      <th>Username</th>\n",
              "      <th>Like Count</th>\n",
              "      <th>Display Name</th>\n",
              "      <th>Clean Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>COVID-19 vaccine required for anyone working a...</td>\n",
              "      <td>tomjgalloway14</td>\n",
              "      <td>10</td>\n",
              "      <td>Tom Galloway</td>\n",
              "      <td>covid19 vaccine required for anyone working at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>trust me, covid hurt. you won't survive withou...</td>\n",
              "      <td>sh_amx_</td>\n",
              "      <td>0</td>\n",
              "      <td>sham üáµüá∏</td>\n",
              "      <td>trust me covid hurt you will not survive witho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>@SammieJack3 Soros Open Society\\nClinton Found...</td>\n",
              "      <td>seve99998</td>\n",
              "      <td>3</td>\n",
              "      <td>seve99999</td>\n",
              "      <td>soros open society\\nclinton foundation \\ngate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>Scotland finally doing QR digital based covid ...</td>\n",
              "      <td>mwhiteside</td>\n",
              "      <td>7</td>\n",
              "      <td>Matthew Whiteside</td>\n",
              "      <td>scotland finally doing qr digital based covid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>@10DowningStreet \\nDear Prime Minister I hope ...</td>\n",
              "      <td>barsh52227438</td>\n",
              "      <td>1</td>\n",
              "      <td>barsh</td>\n",
              "      <td>\\ndear prime minister i hope that it is you t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921236</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>@BCinKW I got Covid. Made my own antibodies. J...</td>\n",
              "      <td>tripodcat73</td>\n",
              "      <td>0</td>\n",
              "      <td>President-Elect Rob</td>\n",
              "      <td>i got covid made my own antibodies joe can ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921237</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>The SNP‚Äôs Covid vaccine passport app launched ...</td>\n",
              "      <td>ScotTories</td>\n",
              "      <td>509</td>\n",
              "      <td>Scottish Conservatives</td>\n",
              "      <td>the snps covid vaccine passport app launched a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921238</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>@Clarence333333 @willcain That‚Äôll happen with ...</td>\n",
              "      <td>mr11rings</td>\n",
              "      <td>0</td>\n",
              "      <td>Mr. 11 Rings(+2 as coach üçÄ)</td>\n",
              "      <td>that will happen with or without people taki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921239</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>50,000 Patients Died Soon In 14 Days After Get...</td>\n",
              "      <td>sunfellow</td>\n",
              "      <td>1</td>\n",
              "      <td>sunfellow</td>\n",
              "      <td>5000 patients died soon in 14 days after getti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921240</th>\n",
              "      <td>2021-09-30</td>\n",
              "      <td>COVID vaccines: time to confront anti-vax aggr...</td>\n",
              "      <td>SheilaBarnhart6</td>\n",
              "      <td>0</td>\n",
              "      <td>Sheila Barnhart</td>\n",
              "      <td>covid vaccines time to confront antivax aggres...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>921241 rows √ó 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Datetime  ...                                         Clean Text\n",
              "0       2021-09-01  ...  covid19 vaccine required for anyone working at...\n",
              "1       2021-09-01  ...  trust me covid hurt you will not survive witho...\n",
              "2       2021-09-01  ...   soros open society\\nclinton foundation \\ngate...\n",
              "3       2021-09-01  ...  scotland finally doing qr digital based covid ...\n",
              "4       2021-09-01  ...   \\ndear prime minister i hope that it is you t...\n",
              "...            ...  ...                                                ...\n",
              "921236  2021-09-30  ...   i got covid made my own antibodies joe can ta...\n",
              "921237  2021-09-30  ...  the snps covid vaccine passport app launched a...\n",
              "921238  2021-09-30  ...    that will happen with or without people taki...\n",
              "921239  2021-09-30  ...  5000 patients died soon in 14 days after getti...\n",
              "921240  2021-09-30  ...  covid vaccines time to confront antivax aggres...\n",
              "\n",
              "[921241 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHTKq4pJkfi2"
      },
      "source": [
        "df_sep.to_csv('sep_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O90vYgNRkidj",
        "outputId": "baaef05c-8edc-42c4-901c-393aa611a7be"
      },
      "source": [
        "df_sep.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Datetime        921241\n",
              "Text            921241\n",
              "Username        921241\n",
              "Like Count      921241\n",
              "Display Name    921202\n",
              "Clean Text      921241\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvWmDzbFkkFT",
        "outputId": "db9cad65-56a7-44d5-a78f-e29018197ade"
      },
      "source": [
        "datatypes = df_sep.dtypes\n",
        "datatypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Datetime        object\n",
              "Text            object\n",
              "Username        object\n",
              "Like Count       int64\n",
              "Display Name    object\n",
              "Clean Text      object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}